---
title: "English Premier League Exploratory Analysis"
author: "Jai D. Bansal"
output: word_document
---


I've played soccer for over a decade, so I thought it would be fun to do some analysis on professional soccer matches. I was able to find English Premier League match data from the 2003/2004 season to the 2015/2016 season (*http://www.football-data.co.uk/englandm.php*)

This document contains results, plots, and my own interpretation. The underlying code is suppressed for clarity but can be viewed in *premier_league_exploratory_analysis.Rmd*.

```{r prep_import_and_clean, include = F, echo = F}
  
  # Load packages.
  library(data.table)
  library(qdapRegex)
  library(ggplot2)
  library(reshape2)
  library(randomForest)
  library(caret)
  library(e1071)

  # Import data.
  # The working directory should be the 'premier-league-data-analysis' folder.
  premier_data = data.table(read.csv('premier_data.csv', 
                                     header = T, 
                                     stringsAsFactors = F))

  # Change 'Date' to date format.
  # This was done in 'data_aggregation_and_cleaning.R' but must be done again since 'Date' gets imported as a 'character' vector.
  premier_data$Date = as.Date(premier_data$Date, 
                              '%Y-%m-%d')
  
  # Make all text fields lowercase and remove extra whitespace.
  premier_data$HomeTeam = rm_white(tolower(premier_data$HomeTeam))
  premier_data$AwayTeam = rm_white(tolower(premier_data$AwayTeam))
  premier_data$Referee = rm_white(tolower(premier_data$Referee))

  # Remove features from 'premier_data' and put them into 'feature_data'.
  # 'feature_data' is used in the 'Match Outcome Prediction' section.
  # I do this to remove clutter and allow the data to be viewed more easily.
  feature_data = premier_data[, .(FTR, home_team_wins, away_team_wins, home_avg_goals, away_avg_goals, home_avg_sot, away_avg_sot, home_avg_corner, away_avg_corner, home_avg_foul, away_avg_foul, home_avg_yellow, away_avg_yellow, home_avg_red, away_avg_red)]
  premier_data = premier_data[, .(Date, HomeTeam, AwayTeam, FTHG, FTAG, FTR, Referee, HS, AS, HST, AST, HC, AC, HF, AF, HY, AY, HR, AR, season_num)]

```

### DATA SAMPLE:

Here's the first few rows of data and what all the columns stand for. This is a pretty rich data set containing not just final scores but also some deeper metrics. 

I've removed columns that aren't used in the analysis (half time goals and results, total shots, and some betting odds). These might be used in future iterations.

```{r view_data, echo = F}

  # View data sample.
  head(premier_data, 3)

```

The abbreviated columns stand for:   

* __FTHG__: home team goals at end of match   
* __FTAG__: away team goals at end of match   
* __FTR__: match result ([h, a, d] denote [home team victory, away team victory, draw] respectively)   
* __HST__: home team shots on target   
* __AST__: away team shots on target   
* __HC__: home team corner kicks   
* __AC__: away team corner kicks   
* __HF__: home team fouls   
* __AF__: away team fouls   
* __HY__: home team yellow cards   
* __AY__: away team yellow cards   
* __HR__: home team red cards   
* __AR__: away team red cards   
* __season_num__: added in *data_aggregation_and_cleaning.R* to indicate which season a match occurred in

### SUMMARY STATISTICS:

* __First Match Date__: `r min(premier_data$Date)`
* __Last Match Date__: `r max(premier_data$Date)`
* __Number of Seasons__: `r length(unique(premier_data$season_num))`
* __Number of Matches__: `r nrow(premier_data)`
* __Number of Teams__: `r length(unique(premier_data$HomeTeam))`
* __Number of Referees__: `r length(unique(premier_data$Referee))`
* __% of Matches Won by Home Team__: `r round(100 * nrow(premier_data[FTR == 'h']) / nrow(premier_data), 2)`
* __% of Matches Won by Away Team__: `r round(100 * nrow(premier_data[FTR == 'a']) / nrow(premier_data), 2)`
* __% of Matches Ending in a Draw__: `r round(100 * nrow(premier_data[FTR == 'd']) / nrow(premier_data), 2)`

### RELEGATION:

At the end of each season, the bottom 3 teams in the standings are relegated to a lower division (the English Football League Championship).
The English Premier League (EPL) is the top English league, so teams cannot be "promoted" out of the league.
Below is the distribution of how many seasons the teams in the dataset have spent in the EPL.

```{r relegation_1, echo = F, include = F}
  
    # I need to do this section in 2 chunks to get the output correct.

    # Count seasons played by each team.
    premier_data[, 
                 seasons_played := length(unique(season_num)), 
                 by = 'HomeTeam']

```

```{r relegation_2, echo = F}

    # Create unique table with 1 row per team.
    seasons = premier_data[, .(HomeTeam, seasons_played)]
    seasons = unique(seasons)
    
    # Create table with number of teams and how many seasons they played.
    # I use 'data.frame' here instead of 'data.table' to get the output to look how I want it.
    seasons_result = data.frame(table(seasons$seasons_played))
    setnames(seasons_result, 
             names(seasons_result), 
             c('Seasons', 'Teams'))

    # Show 'seasons_result'.
    print(seasons_result)
    
```

Out of `r length(unique(premier_data$HomeTeam))` teams, 6 (`r round(100 * 6/38, 1)`%) only spent 1 year in the EPL.
8 teams (`r round(100 * 8/38, 1)`%) spent 13 years, the longest time possible in this data set.
The average number of seasons in the EPL is `r round(mean(seasons$seasons_played), 1)`.
It's interesting that relatively many clubs played 13 seasons and 1 season in the EPL, with relatively few clubs playing the number of seasons in between.

There are many teams that break into the EPL from the division beneath...and are promptly relegated the following year. Getting to the EPL is apparently only the first part of the battle.    

On the other end, there are a group of dominant teams that are rarely relegated.
Looking at the clubs that played all 13 seasons in the EPL (`r seasons[seasons_played == 13]$HomeTeam`) reveals many well-known (and wealthy) teams.

**Further Exploration**:

* What are the differences between teams that were promoted to the EPL and stayed there as opposed to teams that were promoted and then relegated the next year?
* What features distinguish the teams that are rarely/never relegated? Are they in this data set or external data?

### REFEREES:

Let's take a closer look at the people we love to hate. I'm interested in seeing summary statistics about the refs as well as whether some officials are more stringent/lenient than others. Recall that there are `r length(unique(premier_data$Referee))` referees in this data.

```{r referee_analyis, echo = F, include = F}

  # Create a separate data table for this analysis.
  foul_data = premier_data[, .(HomeTeam, AwayTeam, FTR, Referee, HF, AF, HY, AY, HR, AR)]
    
  # Compute the total number of fouls, yellow cards, and red cards for each match.
  foul_data$match_foul = foul_data$HF + foul_data$AF
  foul_data$match_yellow = foul_data$HY + foul_data$AY
  foul_data$match_red = foul_data$HR + foul_data$AR
  
  # Count total fouls, yellow cards, red cards, and matches officiated for each referee.
  foul_data[, 
            ref_matches := .N, 
            by = Referee]
  foul_data[, ref_foul := sum(match_foul), 
             by = Referee]
  foul_data[, ref_yellow := sum(match_yellow), 
             by = Referee]
  foul_data[, ref_red := sum(match_red), 
             by = Referee]
  
  # Find the average number of fouls, yellow cards, and red cards given by each referee per match.
  foul_data[, 
            avg_ref_foul := ref_foul / ref_matches, 
            by = Referee]
  foul_data[, 
            avg_ref_yellow := ref_yellow / ref_matches, 
            by = Referee]
  foul_data[, 
            avg_ref_red := ref_red / ref_matches, 
            by = Referee]
  
  # Create data table with 1 row per referee.
  referee_only = foul_data[, .(Referee, ref_matches, avg_ref_foul, avg_ref_yellow, avg_ref_red)]
  setkey(referee_only)
  referee_only = unique(referee_only)
  
  # Create 'exp_ref' for refs with 10 or more matches.
  exp_ref = referee_only[ref_matches >= 10]

```
**Minimum # of Matches Officiated**: `r round(quantile(referee_only$ref_matches, 0), 2)`    
**Median # of Matches Officiated**: `r round(quantile(referee_only$ref_matches, 0.5), 2)`    
**Maximum # of Matches Officiated**: `r round(quantile(referee_only$ref_matches, 1), 2)`   

In terms of experience, there are a wide variety of officials. An EPL season for a team is roughly 38 games. Using this metric, the median referee has worked for `r round(quantile(referee_only$ref_matches, 0.5) / 38, 2)` seasons and the most experienced referee has been through `r round(quantile(referee_only$ref_matches, 1) / 38, 2)` seasons.

For the rest of this section, I'll only look at referees with 10 or more officiated matches to avoid outlier officials with little experience. There are a few officials who only have 1 match in the data but are near the top or bottom in terms of fouls/yellow cards/red cards handed out.

**Minimum Avg. Fouls Called per Match (Experienced Officials)**: `r round(quantile(exp_ref$avg_ref_foul, 0), 2)`   
**Median Avg. Fouls Called per Match (Experienced Officials)**: `r round(quantile(exp_ref$avg_ref_foul, 0.5), 2)`   
**Maximum Avg. Fouls Called per Match (Experienced Officials)**: `r round(quantile(exp_ref$avg_ref_foul, 1), 2)`   

The variation in average fouls per match doesn't look too extreme to me. Soccer fouls are quite common and often tactically necessary.

**Minimum Avg. Yellow Cards Given per Match (Experienced Officials)**: `r round(quantile(exp_ref$avg_ref_yellow, 0), 2)`   
**Median Avg. Yellow Cards Given per Match (Experienced Officials)**: `r round(quantile(exp_ref$avg_ref_yellow, 0.5), 2)`   
**Maximum Avg. Yellow Cards Given per Match (Experienced Officials)**: `r round(quantile(exp_ref$avg_ref_yellow, 1), 2)`   

This distribution is, in absolute terms, much smaller than fouls called. However, considering that 2 yellow cards gets you ejected from a match, these results are more noteworthy. I find it surprising that the official at the top of the distribution is often handing out enough yellow cards to fully eject 2 players per game.

**Minimum Avg. Red Cards Given per Match (Experienced Officials)**: `r round(quantile(exp_ref$avg_ref_red, 0), 2)`   
**Median Avg. Red Cards Given per Match (Experienced Officials)**: `r round(quantile(exp_ref$avg_ref_red, 0.5), 2)`   
**Maximum Avg. Red Cards Given per Match (Experienced Officials)**: `r round(quantile(exp_ref$avg_ref_red, 1), 2)`   

Red cards, are perhaps reassuringly, pretty rare. Note that I'm not sure how this red card data is tallied. Particularly egregious fouls get an automatic red. But I'm unsure if a player's second yellow card is counted as a red.

**Further Exploration**:

* Is there a correlation between referee experience and the number of fouls/yellow cards/red cards handed out?
* Are the referees at the top/bottom of the foul/yellow card/red card distribution actually more strict/lenient? Or did they just end up officiating more matches betweens teams that were more likely to commit fouls?
* Do officials besides the main referee have any effect on these foul and red/yellow card values?

### BEST OF THE BEST AND WORST OF THE BEST:

I want to look at the winning percentages over time of a subset of teams. In particular, I use 4 teams so the plot is readable. I also only use teams that have been in the data for the entire 13 years. This is an important bias. Teams that have never been relegated are consistently above a certain quality. Simply put, they are the best teams. Out of these best teams, I look at the top (best of the best) and bottom (worst of the best) 2.

Win percentage is defined as (# of wins / # of matches).

```{r best_worst, fig.width = 6, fig.height = 4.5, echo = F}
  
    # Remove unneeded objects.
    rm(referee_only, exp_ref)
   
    # Create 'winner' column in 'premier_data'. This helps with counting below.
    premier_data$winner = ifelse(premier_data$FTR == 'h', premier_data$HomeTeam, 
                                 ifelse(premier_data$FTR == 'a', premier_data$AwayTeam, 'draw'))
    
    # Create 'team_season_list' of all teams in all games specified by season. This helps with counting below.
    home_team_seasons = premier_data[, .(HomeTeam, season_num)]
    away_team_seasons = premier_data[, .(AwayTeam, season_num)]
    setnames(home_team_seasons, 
             names(home_team_seasons), 
             c('team', 'season_num'))
    setnames(away_team_seasons, 
             names(away_team_seasons), 
             c('team', 'season_num'))
    team_season_list = rbind(home_team_seasons, away_team_seasons)
    rm(home_team_seasons, away_team_seasons)

    # Restrict 'seasons' from above to only teams that played 13 seasons.
    seasons = seasons[seasons_played == 13]
    
    # Add 'total_wins','total_games', and a wins and games column for each season to 'seasons'. They're blank for now.
    seasons$total_win_percent = rep(NA, 
                                    nrow(seasons))
    seasons$s1_win_percent = rep(NA, 
                                 nrow(seasons))
    seasons$s2_win_percent = rep(NA, 
                                 nrow(seasons))
    seasons$s3_win_percent = rep(NA, 
                                 nrow(seasons))
    seasons$s4_win_percent = rep(NA, 
                                 nrow(seasons))
    seasons$s5_win_percent = rep(NA, 
                                 nrow(seasons))
    seasons$s6_win_percent = rep(NA, 
                                 nrow(seasons))
    seasons$s7_win_percent = rep(NA, 
                                 nrow(seasons))
    seasons$s8_win_percent = rep(NA, 
                                 nrow(seasons))
    seasons$s9_win_percent = rep(NA, 
                                 nrow(seasons))
    seasons$s10_win_percent = rep(NA, 
                                  nrow(seasons))
    seasons$s11_win_percent = rep(NA, 
                                  nrow(seasons))
    seasons$s12_win_percent = rep(NA, 
                                  nrow(seasons))
    seasons$s13_win_percent = rep(NA, 
                                  nrow(seasons))
    
    # Fill in all empty 'season' values.
    # I use a loop here (and few other times throughout the analysis).
    # Vectorized operations are much faster in R, but the structure of the data makes them challenging to implement.
    # The data set is small enough that running loops seems to be manageable.
    for (i in 1:nrow(seasons))
      {
        
        # Fill in win values.
        seasons$total_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i]]) / nrow(team_season_list[team == seasons$HomeTeam[i]])
        seasons$s1_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 1]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 1])
        seasons$s2_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 2]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 2])
        seasons$s3_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 3]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 3])
        seasons$s4_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 4]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 4])
        seasons$s5_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 5]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 5])
        seasons$s6_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 6]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 6])
        seasons$s7_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 7]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 7])
        seasons$s8_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 8]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 8])
        seasons$s9_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 9]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 9])
        seasons$s10_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 10]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 10])
        seasons$s11_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 11]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 11])
        seasons$s12_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 12]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 12])
        seasons$s13_win_percent[i] = 100 * nrow(premier_data[winner == seasons$HomeTeam[i] & season_num == 13]) / nrow(team_season_list[team == seasons$HomeTeam[i] & season_num == 13])
      
      }
    rm(i)
  
    # Order 'seasons' by 'total_win_percent'.
    seasons = seasons[order(total_win_percent, 
                            decreasing = T)]
    
    # Limit 'seasons' to top and bottom 2 teams.
    # Any more is hard to read on the plot.
    seasons = seasons[c(1:2, ((nrow(seasons) - 1):nrow(seasons))),]
    
    # Remove 'seasons_played' and 'total_win_percent' from 'seasons'. It's not needed.
    seasons$seasons_played = NULL
    seasons$total_win_percent = NULL
    
    # Rename 'seasons' columns for ease of plotting.
    setnames(seasons, 
             names(seasons),
             c('Team', '03-04', '04-05', '05-06', '06-07', '07-08', '08-09', '09-10', '10-11', '11-12', '12-13', '13-14', '14-15', '15-16'))
    
    # Rename 'seasons$Team' with proper team names.
    seasons$Team = c('Man. United', 'Chelsea', 'Everton', 'Aston Villa')
    
    # Reshape data for plotting.
    seasons_reshape = melt(seasons, 
                           id = 'Team')
    
    # Create plot.
    ggplot(data = seasons_reshape, 
           aes(x = variable, 
               y = value, 
               group = Team,
               color = Team)) +
      geom_line() +
      geom_point() +
      ylim(c(0, 100)) +
      ggtitle('Win Percentage of Selected Teams') +
      xlab('Season') +
      ylab('Win Percentage') +
      theme(axis.text.x = element_text(color = 'black', 
                                   size = 11, 
                                   angle = 45), 
      axis.text.y = element_text(color = 'black', 
                                   size = 13), 
      axis.title.x = element_text(size = 13),
      axis.title.y = element_text(size = 13,
                                  vjust = 1),
      axis.ticks = element_blank(),
      plot.title = element_text(face = 'bold', 
                                size = 13), 
      legend.title = element_text(size = 12),
      legend.text = element_text(size = 12))
     
```

Given that none of these 4 teams are ever relegated, I find it surprising that Everton and Aston Villa (especially Aston Villa) consistently win less than 50% of their matches. It seems that these clubs haven't done well recently, but haven't done poorly enough to be relegated. Finally, it's interesting that all 4 teams do worse, in some cases much worse, in the 2015-2016 season than the 2014-2015 season.

**Further Exploration**:

* Conduct this analysis with more teams.
* Why do these teams all do worse in 2015-2016 than in 2014-2015? What teams did well instead and why?

### CLUSTERING BY PLAYING STYLE:

I think it's interesting to think about what the data tells us about how different teams play. There are a variety of features relating to what could be called a team's playing style.

Below, I use K-means to cluster all `r length(unique(premier_data$HomeTeam))` teams in the data by average fouls per game and average shots on net percentage per game. The latter is computed by dividing the shots on net by total shots. I only use 2 dimensions to allow easy visualization. Since different teams have spent different amounts of time in the EPL, it's important to use average quantities as opposed to totals.

Numerical and visual descriptions of the clusters are below.

```{r clustering, echo = F}
   
    # Clean up previous section artifacts.
  rm(seasons, team_season_list, seasons_reshape)
  
  # Create data table for clustering analysis with 1 row per team.
  cluster_data = data.table(team = unique(premier_data$HomeTeam), 
                            home_games = rep(NA, length(unique(premier_data$HomeTeam))), 
                            away_games = rep(NA, length(unique(premier_data$HomeTeam))),
                            home_fouls = rep(NA, length(unique(premier_data$HomeTeam))),
                            away_fouls = rep(NA, length(unique(premier_data$HomeTeam))),
                            home_shots_on_target = rep(NA, length(unique(premier_data$HomeTeam))),
                            away_shots_on_target = rep(NA, length(unique(premier_data$HomeTeam))),
                            home_shots = rep(NA, length(unique(premier_data$HomeTeam))),
                            away_shots = rep(NA, length(unique(premier_data$HomeTeam))))

  # Fill in 'cluster_data'.
  for (i in 1:nrow(cluster_data))
    {
    
      cluster_data$home_games[i] = nrow(premier_data[HomeTeam == cluster_data$team[i]])
      cluster_data$away_games[i] = nrow(premier_data[AwayTeam == cluster_data$team[i]])
      cluster_data$home_fouls[i] = sum(premier_data[HomeTeam == cluster_data$team[i]]$HF) 
      cluster_data$away_fouls[i] = sum(premier_data[AwayTeam == cluster_data$team[i]]$AF)
      cluster_data$home_shots_on_target[i] = sum(premier_data[HomeTeam == cluster_data$team[i]]$HST)
      cluster_data$away_shots_on_target[i] = sum(premier_data[AwayTeam == cluster_data$team[i]]$AST)
      cluster_data$home_shots[i] = sum(premier_data[HomeTeam == cluster_data$team[i]]$HS)
      cluster_data$away_shots[i] = sum(premier_data[AwayTeam == cluster_data$team[i]]$AS)
  
  }
  rm(i)
  
  # Compute total games, fouls, shots on target, and shots (home quantity + away quantity).
  cluster_data$games = cluster_data$home_games + cluster_data$away_games
  cluster_data$fouls = cluster_data$home_fouls + cluster_data$away_fouls
  cluster_data$shots_on_target = cluster_data$home_shots_on_target + cluster_data$away_shots_on_target
  cluster_data$shots = cluster_data$home_shots + cluster_data$away_shots
  
  # Compute average fouls per game and average shot percentage per game for each team.
  cluster_data$avg_fouls = cluster_data$fouls / cluster_data$games
  cluster_data$avg_shots_on_target_percent = 100 * cluster_data$shots_on_target / cluster_data$shots
  
  # Scale 'avg_fouls' and 'avg_shots_on_target_percent'. This should be done prior to clustering.
  # I do this separately so I can later access the mean and standard deviation of scaling.
  avg_fouls_scaled = scale(cluster_data$avg_fouls)
  avg_shots_on_target_percent_scaled = scale(cluster_data$avg_shots_on_target_percent)
  
  # Add 'avg_fouls_scaled' and 'avg_shots_on_target_percent_scaled' to 'cluster_data'.
  cluster_data$avg_fouls_scaled = avg_fouls_scaled
  cluster_data$avg_shots_on_target_percent_scaled = avg_shots_on_target_percent_scaled
  
  # Conduct K-means clustering with 4 clusters.
  cluster_kmeans = kmeans(cluster_data[, .(avg_fouls_scaled, avg_shots_on_target_percent_scaled)], 
                          centers = 4)
  
  # Convert cluster centers back into unscaled values.
  cluster_centers = data.table(cluster_kmeans$centers)
  cluster_centers$avg_fouls_scaled = (cluster_centers$avg_fouls_scaled * attr(avg_fouls_scaled, 'scaled:scale')) + 
                                      attr(avg_fouls_scaled, 'scaled:center')
  cluster_centers$avg_shots_on_target_percent_scaled = (cluster_centers$avg_shots_on_target_percent_scaled * attr(avg_shots_on_target_percent_scaled, 'scaled:scale')) + attr(avg_shots_on_target_percent_scaled, 'scaled:center')
  
  # Add clusters to 'cluster_data'.
  cluster_data$clusters = cluster_kmeans$cluster
  
  # View cluster means.
  cluster_centers$avg_fouls_scaled = round(cluster_centers$avg_fouls_scaled, 1)
  cluster_centers$avg_shots_on_target_percent_scaled = round(cluster_centers$avg_shots_on_target_percent_scaled, 1)
  setnames(cluster_centers, names(cluster_centers), c('Average Fouls', 'Avg. Shots on Net %'))
  cluster_centers
  
  # Plot clusters.
  ggplot(data = cluster_data[, .(avg_fouls, avg_shots_on_target_percent, clusters)], 
         aes(x = avg_fouls, 
             y = avg_shots_on_target_percent, 
             group = as.character(clusters),
             color = as.character(clusters))) +
    geom_point(size = 2, 
               aes(shape = as.character(clusters))) +
    scale_color_manual(values = c('red', 'blue', 'yellow', 'black')) +
    theme(axis.text.x = element_text(color = 'black', 
                                     size = 13), 
          axis.text.y = element_text(color = 'black', 
                                     size = 13), 
          axis.title.x = element_text(face = 'bold', 
                                    size = 13),
          axis.title.y = element_text(face = 'bold',
                                      size = 13, 
                                      vjust = 1),
          axis.ticks = element_blank(),
          plot.title = element_text(face = 'bold', 
                                    size = 13)) +
    guides(colour = F, 
           shape = F) +
    ggtitle('Clustered EPL Teams') +
    xlab('Average Fouls Per Game') +
    ylab('Avg. Shots on Net % Per Game')
 
```

Oddly enough, RMarkdown output has different colors/shapes in different positions everytime This needs to be filled in properly in the final version. The  <> cluster represents teams with many fouls and relatively high shot accuracy. The <> cluster commits less fouls than the <> cluster and has a slightly lower shot accuracy. The <> cluster is characterized by a lower shot accuracy. The <> cluster admittedly looks like a miscellaneous clean-up cluster, but is generally characterized by low shot accuracy and fouls.

**Further Exploration**:

* Add more dimensions to the clustering.
* See if certain playing styles are correlated with winning/losing records.
* Do teams that never get relegated tend to have similar styles?
* Do teams that are promoted into the EPL tend to have similar styles?

### MATCH OUTCOME PREDICTION - CAN I PREDICT WINNERS?:

This might be the most interesting question that can be asked with sporting event data: can I predict the winner?

I'll use a random forest to predict match outcomes (home win, away win, draw).   
I use features (wins, average goals per match, average on-target shots per match, average corner kicks per match, average fouls per match, average yellow cards per match, average red cards per match) computed for both teams from the **same season** only.   

For example, suppose team A plays team B in season 2. I would compute all features for both teams using only previous matches **in season 2**.   
This method means I cannot make predictions for a team's 1st match of the season.  
For a team's 2nd match of the season, I will make predictions using only data from that team's 1st match.
For a team's 10th match, I obtain predictions using data from the 1st 9 matches.   
So, each team's features are being updated every match.

```{r match_outcome_prediction_initial, echo = F}
  
  # Remove unneeded objects:
  rm(avg_fouls_scaled, avg_shots_on_target_percent_scaled, cluster_centers, cluster_data, seasons_result, cluster_kmeans)

  # This section uses 'feature_data'.
  # Recall that, for each match, features from previous matches in the same season for the relevant teams are used for prediction.
  # Thus, for a team's first match of the season, no prediction is possible.
  # I remove these rows from 'feature_data'.
  feature_data = feature_data[is.na(home_avg_goals) == F & is.na(away_avg_goals) == F]

  # For classification, 'feature_data$FTR' must be of type 'factor'.
  feature_data$FTR = as.factor(feature_data$FTR)

  # Create training and test sets from 'feature_data'.
  # I use ~70% of the data for training.
  # The remaining ~25% of the data is used as a test set.

    # Set seed for reproducibility.
    set.seed(25)
  
    # Randomly choose rows to be in training set.
    training_rows = sample(1:nrow(feature_data), 
                           round(0.70 * nrow(feature_data)), 
                           replace = F)

    # Create training and test sets.
    feature_data_train = feature_data[training_rows]
    feature_data_test = feature_data[-training_rows]

  # Specify model.
  match_model = FTR ~.

  # Set seed for reproducibility.
  set.seed(25)

  # Create random forest.
  match_rf = randomForest(match_model, 
                          data = feature_data_train,
                          ntree = 251, 
                          importance = T)

  # Add OOB (out-of-bag) training set predictions to 'feature_data_train'.
  # The OOB score can be substituted for a cross validation score when using random forest.
  feature_data_train$oob_pred = match_rf$predicted

  # Generate predictions for training and test sets.
  feature_data_train$train_pred = data.table(predict(match_rf, 
                                               newdata = feature_data_train, 
                                               type = 'response'))
  feature_data_test$test_pred = data.table(predict(match_rf, 
                                               newdata = feature_data_test, 
                                               type = 'response'))
  
```

Using random forest out of the box, I obtain:   
- training set error of **`r 100 * round(nrow(feature_data_train[FTR != train_pred]) / nrow(feature_data_train), 4)`%**   
- OOB (out-of-bag) sample error of **`r 100 * round(nrow(feature_data_train[FTR != oob_pred]) / nrow(feature_data_train), 4)`%** (I use this as a cross-validation error)   
- test set error of **`r 100 * round(nrow(feature_data_test[FTR != test_pred]) / nrow(feature_data_test), 4)`%**

The baseline model I can compare against is predicting a home team win every time (yielding a test set error rate of **`r 100 * round(nrow(feature_data_test[FTR != 'h']) / nrow(feature_data_test), 4)`%**).

So, the random forest model is slightly better than always predicting the home team wins. Now, I'll try to beat the initial model using feature selection and parameter tuning. The training set error (0%) indicates I have an overfitting problem.

```{r feature_selection, echo = F}

  # This section is commented out because it takes too long to run every time.
  # I include findings from when this code was run, it's just not run every time.
  # These results also vary with the random seed that is set.

  # Set seed for reproducibility.
  #set.seed(25)

  # Define the control function for random forest functions.
  # This will be used to specify some of the parameters of the feature selection.
  #control = rfeControl(functions = rfFuncs,             # random forest functions
  #                   method = 'cv',                     # cross validation
  #                   number = 5, ,                      # number of folds
  #                   allowParallel = T, 
  #                   verbose = T)

  # Conduct backwards recursive feature selection.
  #results = rfe(x = feature_data_train[, .(home_team_wins, away_team_wins, home_avg_goals, away_avg_goals, home_avg_sot, away_avg_sot, home_avg_corner, 
  #                                       away_avg_corner, home_avg_foul, away_avg_foul, home_avg_yellow, away_avg_yellow, home_avg_red, away_avg_red)], 
  #              y = feature_data_train$FTR, 
  #              sizes = c(1:14), 
  #              rfeControl = control)

  # View plot of results.
  #plot(results)

  # See the best predictors
  #predictors(results)

```

Backwards recursive feature selection indicates that using 12 features is slightly better than using the entire feature set.     
I'll use this 12 feature model for parameter tuning.

Note that I ran feature selection code and noted the results, but it is now commented out and included for reference. It makes the generation of this document take way too long.

The 2 excluded features are *away_avg_foul* and *home_avg_yellow*.

```{r parameter_tuning, echo = F}

  # This section is commented out because it takes too long to run every time.
  # I include findings from when this code was run, it's just not run every time.

  # Remove unneeded objects.
  #rm(training_rows)

  # Specify model found from feature selection.
  #best_model = FTR ~ home_team_wins + away_team_wins + home_avg_goals + away_avg_goals + home_avg_sot + away_avg_sot + home_avg_corner + away_avg_corner + home_avg_foul + away_avg_yellow + away_avg_red + home_avg_red

  # Set seed for reproducibility.
  #set.seed(25)

  # Create 'parameters' data table containing all of the parameters combinations I want to try.
  #parameters = expand.grid(ntree = c(251, 501, 751), 
  #                         mtry = c(2, 3, 4, 5), 
  #                         nodesize = c(1, 3, 5, 7, 12, 17, 20))

  # Create columns in 'parameters' for training, OOB, and test set error.
  #parameters$train_error = rep(NA, 
  #                             nrow(parameters))
  #parameters$oob_error = rep(NA, 
  #                           nrow(parameters))
  #parameters$test_error = rep(NA, 
  #                            nrow(parameters))

  # Loop through the rows of 'parameters' and fit a random forest for each combination of parameters.
  #for (i in 1:nrow(parameters))
  #  {
    
      # Keep track of loop progress.
  #    print(i)
    
      # Create random forest model.
  #    rf_parameter_test = randomForest(best_model, 
  #                                     data = feature_data_train, 
  #                                     ntree = parameters$ntree[i],
  #                                     mtry = parameters$mtry[i], 
  #                                     nodesize = parameters$nodesize[i])
      
      # Save predictions so I can compute error rates.
      
        # Add OOB training set predictions to 'feature_data_train'.
  #      feature_data_train$oob_pred = rf_parameter_test$predicted
  
        # Generate predictions for training and test sets.
  #      feature_data_train$train_pred = data.table(predict(rf_parameter_test, 
  #                                                         newdata = feature_data_train, 
  #                                                         type = 'response'))
  #      feature_data_test$test_pred = data.table(predict(rf_parameter_test, 
  #                                                       newdata = feature_data_test, 
  #                                                       type = 'response'))
      
      # Compute training, OOB, and test set errors and store values in 'parameters'.
      
        # Training error.
  #      parameters$train_error[i] = nrow(feature_data_train[FTR != train_pred]) / nrow(feature_data_train)
      
        # OOB error.
  #      parameters$oob_error[i] = nrow(feature_data_train[FTR != oob_pred]) / nrow(feature_data_train)
      
        # Test error.
  #      parameters$test_error[i] = nrow(feature_data_test[FTR != test_pred]) / nrow(feature_data_test)  
    
  #  }

```

I tune 3 parameters: # of trees, # of variables considered at each split, and the minimum number of observations in terminal nodes.   
Parameter tuning shows that the best values for [ntree, mtry, nodesize] are [251, 3, 20] respectively. This specification results in a test error of **47.82%**, a modest improvement over the initial model.

The training error for this specification is ~23% (as opposed to 0% for the original model), but obviously the refined model generalizes better.   

Note that I ran parameter tuning code and noted the results, but it is now commented out but included for reference. It makes the generation of this document take way too long.

So, what's the answer to the question that started this section? Yes, technically. The refined model does better than the null model, chance, and the initial model. But would I bet my life savings on it? Definitely not.

**Further Exploration**:

* Run parameter tuning with more parameters
* Other modeling approaches
* Incorporate new features and domain knowledge (possibly betting odds, features that represent injuries, team wealth, etc.)
* Fix matches to ensure 100% accuracy 
   